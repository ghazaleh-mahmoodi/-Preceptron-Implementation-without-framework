{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd4GU-Pf4vd9"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gym\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from gym) (1.19.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from gym) (1.6.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py): started\n",
            "  Building wheel for gym (setup.py): finished with status 'done'\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616831 sha256=cfffa59e863991762d292069beb5c672625f3df8da3121fbee505736512e1520\n",
            "  Stored in directory: c:\\users\\acer\\appdata\\local\\pip\\cache\\wheels\\27\\6d\\b3\\a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "Successfully installed gym-0.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b8Ie-mkhiGjT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "from gym import Env\n",
        "import datetime\n",
        "\n",
        "class FrozenLake(Env):\n",
        "    def __init__(self,studentNum:int=256, nonStationary = False):\n",
        "        self.studentNum = studentNum\n",
        "        self.nonStationary = nonStationary\n",
        "        \n",
        "        np.random.seed(self.studentNum)\n",
        "        self.beginMap = make_map(self.studentNum) #*2\n",
        "        self.beginMap[self.beginMap>1] = 1\n",
        "        self.endMap = make_map(self.studentNum + 100)\n",
        "        \n",
        "        self.changeDir = self.endMap - self.beginMap\n",
        "        self.changeDir *= 1/11000\n",
        "\n",
        "        self.fixedMap = self.beginMap\n",
        "\n",
        "        np.random.seed(datetime.datetime.now().microsecond)\n",
        "        \n",
        "        self.map = copy.deepcopy(self.fixedMap)\n",
        "        self.time = 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.NSreset()\n",
        "        if not self.nonStationary:\n",
        "            self.map = copy.deepcopy(self.fixedMap)\n",
        "            self.time = 0\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    def NSreset(self):\n",
        "        self.time += 1\n",
        "        self.map += self.changeDir\n",
        "\n",
        "        self.map[self.map>0.95]=0.95\n",
        "        self.map[self.map<0.0]=0.0\n",
        "\n",
        "        self.state = (0,0)\n",
        "        self.done = False\n",
        "        return self.state\n",
        "    \n",
        "    def states_transitions(self, state, action):\n",
        "        x = state[0]\n",
        "        y = state[1]\n",
        "        states = np.array([[x,y-1], [x,y+1], [x-1 ,y], [x+1,y] ])\n",
        "\n",
        "\n",
        "        if action == UP:\n",
        "            selected = states[2]\n",
        "        if action == DOWN:\n",
        "            selected = states[3]\n",
        "        if action == RIGHT:\n",
        "            selected = states[1]\n",
        "        if action == LEFT:\n",
        "            selected = states[0]\n",
        "\n",
        "        zero = np.zeros((4,2)).astype(int)\n",
        "        three = (3 * np.ones((4,2))).astype(int)\n",
        "        output = np.maximum(np.minimum(states, three),zero)\n",
        "        output, indices = np.unique(output, axis = 0, return_counts= True)\n",
        "\n",
        "        \n",
        "        selected = np.maximum(np.minimum(selected, three[0]), zero[0])\n",
        "        probs = indices * 0.025\n",
        "        probs[np.argmax(np.sum(selected == output, axis = 1))] += 0.9\n",
        "\n",
        "        return list(zip(output[:,0],output[:,1])), probs\n",
        "    \n",
        "    def possible_consequences(self,action:int,state_now=None):\n",
        "\n",
        "        if state_now==None:\n",
        "            state_now = self.state\n",
        "        state = [state_now[0],state_now[1]]\n",
        "        states, probs = self.states_transitions(state, action)\n",
        "        aa = np.array(states) \n",
        "        fail_probs = self.map[(aa[:,0]),(aa[:,1])]\n",
        "        dones = np.sum(aa == 3, axis = 1) == 2\n",
        "        return states, probs, fail_probs,dones\n",
        "    \n",
        "    def step(self, a:int):\n",
        "        if not (a in range(4)):\n",
        "            raise Exception(\"action is not available!!!\")\n",
        "        \n",
        "        states, probs, fail_probs,dones = self.possible_consequences(a)\n",
        "        \n",
        "        next_idx = np.random.choice(np.arange(len(states)), p = probs)\n",
        "        next_state = states[next_idx]\n",
        "        self.state = tuple(next_state)\n",
        "        \n",
        "        self.done = dones[next_idx]\n",
        "\n",
        "        r = -1\n",
        "\n",
        "        if self.done:\n",
        "            r += 60\n",
        "        elif np.random.rand()< fail_probs[next_idx]:\n",
        "            r -= 15\n",
        "            self.done = True\n",
        "\n",
        "        return (self.state, r, self.done, {})\n",
        "\n",
        "    def render(self,state=None):\n",
        "        if state == None:\n",
        "            state = self.state\n",
        "\n",
        "        out = \"\"\n",
        "        for i in range(4):\n",
        "            out += \"\\n------------------------------\\n| \"\n",
        "            for j in range(4):\n",
        "                if (i,j) == state:\n",
        "                    out += \"\\033[44m{:.3f}\\033[0m | \".format(self.map[i,j])\n",
        "                else :\n",
        "                    out += \"{:.3f} | \".format(self.map[i,j])\n",
        "\n",
        "        out += \"\\n------------------------------\"\n",
        "        print(out)\n",
        "\n",
        "    def environment_states(self):\n",
        "        env_states = []\n",
        "        for state_index in range(16):\n",
        "            s0 = state_index % 4\n",
        "            s1 = state_index//4\n",
        "            env_states.append((s0,s1))\n",
        "        return env_states\n",
        "\n",
        "        \n",
        "def set_max_min(var,maximum,minimum):\n",
        "    return min(max(var,minimum),maximum)\n",
        "\n",
        "def make_map(studentNum):\n",
        "    np.random.seed(studentNum)  \n",
        "    move = np.zeros(6)\n",
        "    idx = np.random.choice(range(6),size=3,replace=False)\n",
        "    move[idx] = 1\n",
        "    point = [0,0]\n",
        "    lowprobs = [tuple(point)]\n",
        "\n",
        "    for m in move:\n",
        "        if m:\n",
        "            point[0] += 1\n",
        "        else:\n",
        "            point[1] += 1\n",
        "        lowprobs.append(tuple(point))\n",
        "    \n",
        "    map = np.random.rand(4,4)\n",
        "    idx = np.array(lowprobs)\n",
        "\n",
        "    map[idx[:,0],idx[:,1]] = 0.001 \n",
        "    map[0,0] = 0.0\n",
        "    map[3,3] = 0.0 \n",
        "\n",
        "    return map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIrM2PNQ4l5G"
      },
      "source": [
        "## Your Student ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JvWeNrBx4or9"
      },
      "outputs": [],
      "source": [
        "STUDENT_NUM = 400722156"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba-NtxEn5LJ0"
      },
      "source": [
        "# HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "54H4VswF5Kot"
      },
      "outputs": [],
      "source": [
        "#%% allowed actions\n",
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3\n",
        "\n",
        "ACTIONS = [LEFT,DOWN,RIGHT,UP]\n",
        "\n",
        "#%% hyperparameters\n",
        "EPISODES = 10000\n",
        "EPSILON = 0.1\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdub8jub5bM9"
      },
      "source": [
        "## Map of environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ME5gllo7g0p7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment with fail probabilities :\n",
            "\n",
            "------------------------------\n",
            "| \u001b[44m0.000\u001b[0m | 0.808 | 0.427 | 0.546 | \n",
            "------------------------------\n",
            "| 0.001 | 0.684 | 0.899 | 0.870 | \n",
            "------------------------------\n",
            "| 0.001 | 0.001 | 0.001 | 0.259 | \n",
            "------------------------------\n",
            "| 0.872 | 0.043 | 0.001 | 0.000 | \n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "environment = FrozenLake(studentNum=STUDENT_NUM)\n",
        "\n",
        "print(\"Environment with fail probabilities :\")\n",
        "environment.render()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z-KEpaeOcAh"
      },
      "source": [
        "## <h2><font color=indigo> Agent Implementation\n",
        "Implement your q-learning (off-policy TD) agent here. You need to utilize the step function provided in the Environment class to interact with frozen lake environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9P-5IZqIeco8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "class Q_Learning:\n",
        "    def __init__(self, id, environment, discount , learning_rate = 0.1 , epsilon = 0.1 ,episodes=10000):\n",
        "        self.id = 400722156\n",
        "        np.random.seed(self.id)\n",
        "        self.environment = environment\n",
        "        self.discount = discount\n",
        "        self.episodes = episodes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = 4\n",
        "        \n",
        "        \n",
        "        self.n_states = len(environment.environment_states())\n",
        "        self.Q = np.zeros((self.n_states, self.n_actions))\n",
        "        \n",
        "        self.state_to_number = self.environment.environment_states()\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        # we e-greedy approach for our action choice\n",
        "       \n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice([i for i in range(self.n_actions)])\n",
        "        else:\n",
        "            state = self.state_to_number.index(state)\n",
        "            actions = np.array([self.Q[(state, a)] for a in range(self.n_actions)])\n",
        "            action = np.argmax(actions)\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "    def learn(self, state, action, reward, new_state):\n",
        "        state = self.state_to_number.index(state)\n",
        "        new_state = self.state_to_number.index(new_state)\n",
        "        \n",
        "        actions = np.array([self.Q[(new_state, a)] for a in range(self.n_actions)])\n",
        "        \n",
        "        # the q-learning equation, calculating state, action values\n",
        "        self.Q[(state, action)] += self.learning_rate * (reward + \\\n",
        "                                    self.discount * \\\n",
        "                                    self.Q[(new_state, np.argmax(actions))] - self.Q[(state, action)])\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        \n",
        "        for i in range(self.episodes):\n",
        "            \n",
        "            state = self.environment.reset()\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "\n",
        "                action = self.choose_action(state)\n",
        "                new_state, reward, done, _ = self.environment.step(action)\n",
        "                self.learn(state, action, reward, new_state)\n",
        "                state = new_state\n",
        "                \n",
        "            if i % 5000 == 0:\n",
        "                print(\"episode \", i)\n",
        "\n",
        "        print(\"finished\")\n",
        "        print()\n",
        "        print(self.Q)\n",
        "        \n",
        "        return self.Q, _\n",
        "\n",
        "    def policy(self, state):\n",
        "        state = self.state_to_number.index(state)\n",
        "        actions = np.array([self.Q[(state, a)] for a in range(self.n_actions)])\n",
        "        return np.argmax(actions)\n",
        "    \n",
        "    def find_best_policy(self):\n",
        "        \n",
        "        state_sequence = []\n",
        "        action_sequence = []\n",
        "\n",
        "        state = self.environment.reset()\n",
        "        done = False\n",
        "        for i in range(10):\n",
        "            state_sequence.append(state)\n",
        "            action = self.policy(state)\n",
        "            action_sequence.append(ACTIONS[action])\n",
        "            next_state, reward, done, _ = self.environment.step(action)\n",
        "            state = next_state\n",
        "            \n",
        "            self.environment.render(state)\n",
        "            if done:\n",
        "                break\n",
        "        return state_sequence, action_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6J4uXQGuTgs"
      },
      "source": [
        "## <h2><font color=indigo> Q Values\n",
        "Return the Q values that your agent learns in here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WYZfiWY6qMch"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode  0\n",
            "episode  5000\n",
            "finished\n",
            "\n",
            "[[ 22.54933906  26.74276804  16.40091051  21.39933627]\n",
            " [ 24.49798767  30.85131586  25.15554469  23.42738719]\n",
            " [ 29.03512516  25.20621412  30.2705394   25.98695389]\n",
            " [ 21.35443003  22.62576451  45.16810099  28.19256763]\n",
            " [  1.89165374  30.05449433  -3.93478053   3.98112647]\n",
            " [ 28.75918169  32.53626122  12.92851318   9.36433128]\n",
            " [ 29.56294134  44.01460407  33.30128771  20.9805798 ]\n",
            " [ 26.88379903  36.4746089   51.87570039  38.27085577]\n",
            " [ -8.          -6.43366301  -8.25        -0.5       ]\n",
            " [ -0.37808241  41.44991919  -4.65043702   0.        ]\n",
            " [ 37.83753954  27.80989139  27.00511458  21.30523034]\n",
            " [ 44.70754012  45.94945405  58.9999999   31.60475767]\n",
            " [ -7.2580509    0.           0.           0.        ]\n",
            " [  7.44347329   0.           0.           0.        ]\n",
            " [ 17.11842416  21.09956298  21.91684221 -15.9921875 ]\n",
            " [  0.           0.           0.           0.        ]]\n"
          ]
        }
      ],
      "source": [
        "agent = Q_Learning('Ghazaleh_Mahmoudi', environment, 0.9 , learning_rate = 0.5 , epsilon = 0.1 ,episodes= 10000)\n",
        "Q , policy = agent.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMojYRGVvAXk"
      },
      "source": [
        "## <h2><font color=darkcyan> Policy\n",
        "Return the optimal policy that your agent learns in here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9EFY3T0r9OHW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------------------------\n",
            "| 0.000 | 0.808 | 0.427 | 0.546 | \n",
            "------------------------------\n",
            "| \u001b[44m0.001\u001b[0m | 0.684 | 0.899 | 0.870 | \n",
            "------------------------------\n",
            "| 0.001 | 0.001 | 0.001 | 0.259 | \n",
            "------------------------------\n",
            "| 0.872 | 0.043 | 0.001 | 0.000 | \n",
            "------------------------------\n",
            "\n",
            "------------------------------\n",
            "| 0.000 | 0.808 | 0.427 | 0.546 | \n",
            "------------------------------\n",
            "| 0.001 | 0.684 | 0.899 | 0.870 | \n",
            "------------------------------\n",
            "| \u001b[44m0.001\u001b[0m | 0.001 | 0.001 | 0.259 | \n",
            "------------------------------\n",
            "| 0.872 | 0.043 | 0.001 | 0.000 | \n",
            "------------------------------\n",
            "\n",
            "------------------------------\n",
            "| 0.000 | 0.808 | 0.427 | 0.546 | \n",
            "------------------------------\n",
            "| 0.001 | 0.684 | 0.899 | 0.870 | \n",
            "------------------------------\n",
            "| 0.001 | \u001b[44m0.001\u001b[0m | 0.001 | 0.259 | \n",
            "------------------------------\n",
            "| 0.872 | 0.043 | 0.001 | 0.000 | \n",
            "------------------------------\n",
            "\n",
            "------------------------------\n",
            "| 0.000 | 0.808 | 0.427 | 0.546 | \n",
            "------------------------------\n",
            "| 0.001 | 0.684 | 0.899 | 0.870 | \n",
            "------------------------------\n",
            "| 0.001 | 0.001 | \u001b[44m0.001\u001b[0m | 0.259 | \n",
            "------------------------------\n",
            "| 0.872 | 0.043 | 0.001 | 0.000 | \n",
            "------------------------------\n",
            "\n",
            "------------------------------\n",
            "| 0.000 | 0.808 | 0.427 | 0.546 | \n",
            "------------------------------\n",
            "| 0.001 | 0.684 | 0.899 | 0.870 | \n",
            "------------------------------\n",
            "| 0.001 | 0.001 | 0.001 | \u001b[44m0.259\u001b[0m | \n",
            "------------------------------\n",
            "| 0.872 | 0.043 | 0.001 | 0.000 | \n",
            "------------------------------\n",
            "\n",
            "------------------------------\n",
            "| 0.000 | 0.808 | 0.427 | 0.546 | \n",
            "------------------------------\n",
            "| 0.001 | 0.684 | 0.899 | 0.870 | \n",
            "------------------------------\n",
            "| 0.001 | 0.001 | 0.001 | 0.259 | \n",
            "------------------------------\n",
            "| 0.872 | 0.043 | 0.001 | \u001b[44m0.000\u001b[0m | \n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "state_sequence, action_sequence = agent.find_best_policy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### step sequence to reach the goal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 0), (1, 0), (2, 0), (2, 1), (2, 2), (3, 2)]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "action_translation = {0: \"LEFT\", 1: \"DOWN\", 2: \"RIGHT\", 3: \"UP\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### action sequence to reach the goal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current state:  (0, 0)  action:  DOWN\n",
            "current state:  (1, 0)  action:  DOWN\n",
            "current state:  (2, 0)  action:  RIGHT\n",
            "current state:  (2, 1)  action:  DOWN\n",
            "current state:  (2, 2)  action:  RIGHT\n",
            "current state:  (2, 3)  action:  DOWN\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(state_sequence)):\n",
        "    print(\"current state: \", state_sequence[i], \" action: \", action_translation[action_sequence[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "StudentName__HW5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "23bf0724a01b6ea9814e66f76182ea78c0ee849a72ca257c0e116bf83bb4960a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
