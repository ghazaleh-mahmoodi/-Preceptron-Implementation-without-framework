{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2asXWhAGF4p"
      },
      "source": [
        "# Install Requireed Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SYKmTJ-h-cws"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyglet in c:\\users\\acer\\anaconda3\\lib\\site-packages (1.5.21)\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "!pip install gym\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install pyglet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aASkSzaRFyJN"
      },
      "source": [
        "# Import Requireed Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wcKzpnsHEGU3"
      },
      "outputs": [],
      "source": [
        "# from gym import wrappers\n",
        "import numpy as np\n",
        "import gym\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "iteration_max = 10000\n",
        "test_max = 2000\n",
        "\n",
        "min_learning_rate = 0.03\n",
        "eps = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def obs_to_state(obs):\n",
        "    \"\"\" Maps an observation to state \"\"\"\n",
        "    state_cos_theta = int(np.digitize(obs[0], Sample_cos_theta))\n",
        "    state_sin_theta = int(np.digitize(obs[1], Sample_sin_theta))\n",
        "    state_theta_dot = int(np.digitize(obs[2], Sample_theta_dot))\n",
        "    return (state_cos_theta, state_sin_theta, state_theta_dot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----- Start Learning -----\n",
            "Iteration 0 -- alpha = 1.000000 Total reward = -1517.\n",
            "Iteration 1000 -- alpha = 0.196874 Total reward = -1219.\n",
            "Iteration 2000 -- alpha = 0.038760 Total reward = -128.\n",
            "Iteration 3000 -- alpha = 0.030000 Total reward = -1383.\n",
            "Iteration 4000 -- alpha = 0.030000 Total reward = -1305.\n",
            "Iteration 5000 -- alpha = 0.030000 Total reward = -870.\n",
            "Iteration 6000 -- alpha = 0.030000 Total reward = -964.\n",
            "Iteration 7000 -- alpha = 0.030000 Total reward = -899.\n",
            "Iteration 8000 -- alpha = 0.030000 Total reward = -1163.\n",
            "Iteration 9000 -- alpha = 0.030000 Total reward = -367.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    \n",
        "    env = gym.make('Pendulum-v1')\n",
        "    env.seed(0)\n",
        "    np.random.seed(0)\n",
        "    \n",
        "    print ('----- Start Learning -----')\n",
        "    \n",
        "    Sample_cos_theta = np.around(np.arange(env.observation_space.low[0], env.observation_space.high[0], 0.1), 1)[1:]\n",
        "    Sample_sin_theta = Sample_cos_theta\n",
        "    Sample_theta_dot = np.around(np.arange(env.observation_space.low[2], env.observation_space.high[2], 1), 0)[1:]\n",
        "\n",
        "    Sample_out = np.around(np.arange(-2, 2.2, 0.2), 1)\n",
        "\n",
        "    q_state_table = np.zeros((len(Sample_cos_theta) + 1, len(Sample_cos_theta) + 1, len(Sample_cos_theta) + 1, len(Sample_out)))\n",
        "    \n",
        "    scores = []\n",
        "    \n",
        "    for i in range(iteration_max):\n",
        "        obs = env.reset()\n",
        "        new_state = obs_to_state(obs)\n",
        "        total_reward = 0\n",
        "\n",
        "        alpha = max(min_learning_rate, 1.0 * (0.85 ** (i//100)))\n",
        "        \n",
        "        for j in range(test_max):\n",
        "            current_state = new_state\n",
        "\n",
        "            #select action : random or using q_state(best action from current state)\n",
        "\n",
        "            if np.random.random() < eps:\n",
        "                action_index = np.random.randint(len(Sample_out))\n",
        "            \n",
        "            else:\n",
        "                action_index = np.argmax(q_state_table[current_state])\n",
        "            \n",
        "            # map index to action value\n",
        "            action = Sample_out[action_index]  \n",
        "            obs, reward, done, _ = env.step([action])\n",
        "            total_reward += reward\n",
        "\n",
        "            new_state = obs_to_state(obs)\n",
        "            q_state_table[current_state][action_index] = q_state_table[current_state][action_index] \\\n",
        "                + alpha * (reward + np.max(q_state_table[new_state])- q_state_table[current_state][action_index])\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        if i % 1000 == 0:\n",
        "            print('Iteration %d -- alpha = %f Total reward = %d.' %(i, alpha, total_reward))    \n",
        "\n",
        "    scores.append(total_reward)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "    frames = []\n",
        "    env = gym.wrappers.Monitor(env, \"./Pendulum-v1_result\", force=True)\n",
        "    obs = env.reset()\n",
        "    while True:\n",
        "        frames.append(env.render(mode = 'rgb_np'))\n",
        "        state = obs_to_state(obs)\n",
        "        action_idx = np.argmax(q_state_table[state])\n",
        "        obs, reward, done, _ = env.step([Sample_out[action_idx]])  # conversion index to value\n",
        "        if done:\n",
        "            print(\"done\")\n",
        "            time.sleep(1)\n",
        "            break\n",
        "\n",
        "    env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "NN_HW6_FuzzyFan(Q2).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "23bf0724a01b6ea9814e66f76182ea78c0ee849a72ca257c0e116bf83bb4960a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
